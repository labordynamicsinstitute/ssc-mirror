<pre>
<b>help mmregress</b> 
-------------------------------------------------------------------------------
<p>
<b><u>Title</u></b>
<p>
    <b>mmregress</b> --  MM-robust regression
<p>
<b><u>Syntax</u></b>
<p>
        <b>mmregress</b> <i>depvar</i> [<i>indepvars</i>] [<i>if</i>] [<i>in</i>] [<b>,</b> <i>options</i>]
<p>
    <i>options</i>                 Description
    -------------------------------------------------------------------------
<p>
      <b><u>noc</u></b><b>onstant</b>            suppress constant term
      <b>eff(</b><i>#</i><b>)</b>                fix the desired efficiency
      <b>dummies(</b><i>dummies</i><b>)</b>      declare dummy variables
      <b>outlier</b>               generate outlyingness measures
      <b>graph</b>                 generate an outlier identification graphical tool
      <b>label(</b><i>varname</i><b>)</b>        label largest outliers according to <i>varname</i>
      <b>replic(</b><i>#</i><b>)</b>             set the number of sub-sampling to consider
      <b>init</b>                  return the initial S (or MS) estimator
<p>
    -------------------------------------------------------------------------
<p>
<b><u>Description</u></b>
<p>
    <b>mmregress</b> fits an MM-estimator of regression of <i>depvar</i> on <i>varlist</i>.  An
    MM-estimator of regression is a robust fitting approach which minimizes a
    (rho) function of the regression residuals which is even, non decreasing
    for positive values and less increasing than the square function. The
    function used here is a Tukey Biweight.  The default Guassian efficiency
    is set to 70% but can be changed by calling the <b>eff</b> option.  The
    Breakdown point is 50%.
<p>
<p>
<p>
<b><u>Options</u></b>
<p>
        +-------+
    ----+ Model +------------------------------------------------------------
<p>
    <b>noconstant</b>; see <b>[R] estimation options</b>.
<p>
<p>
        +-----------+
    ----+ Algorithm +--------------------------------------------------------
<p>
    <b>eff(</b><i>#</i><b>)</b>; The Gaussian efficiency of the MM-estimator can be changed (it
        can to be set to any value between 0.287 and 0.99). Keep however in
        mind that a higher efficiency is associated to a higher bias.
<p>
    <b>dummies(</b><i>dummies</i><b>)</b>; If several dummy variables are present among the
        explanatory variables, the preliminary S-estimator algorithm could
        fail. An MS-estimator can be used instead by declaring the list of
        dummy variables <i>dummies</i> present in the model.
<p>
    <b>graph</b>; Displays a graphic where outliers are flagged according to their
        type.
<p>
    <b>label(</b><i>varname</i><b>)</b>; labels the largest outliers using variable <i>varname</i>. This
        option only works jointly with the graph option. If this option is
        not declared, the label will be the observation linenumber.
<p>
    <b>outlier</b>; Four outlyingness measures are calculated. The first (S_stdres
        or MS_stdres) contains the robust standardized residuals, the second
        (S_outlier or MS_outlier) flags outliers in the vertical dimension
        (i.e. observations associated with robust standardized residual
        larger than 2.25), the third (Robust_distance) contains robust
        distances and the fourth (MCD_outlier) flags outliers in the
        horizontal dimension (i.e. observations associated with robust
        distances larger than the 97.5th percentile of a Chi-quared).
<p>
    <b>replic(</b><i>#</i><b>)</b>; The number of subsets associated to the underlying algorithm
        is set by default using the formula
        replic=log(1-0.99)/log(1-(1-0.2)^(p+1)) where <i>p</i> is the number of
        explanatory variables. This can be changed using the replic option.
<p>
    <b>init</b>; The initial S (or MS) estimator is returned instead of the final
        MM. This is equivalent to setting the efficiency to 0.287.
<p>
<p>
<p>
<p>
<b><u>Saved results</u></b>
<p>
    <b>mmregress</b> saves the following in <b>e()</b>:
<p>
    Scalars   
      <b>e(scale)</b>       robust residual scale
      <b>e(N)</b>           number of observations
      <b>e(df_m)</b>        model degrees of freedom
      <b>e(df_r)</b>        residual degrees of freedom
<p>
    Macros    
      <b>e(cmd)</b>         <b>mmregress</b>
      <b>e(properties)</b>  <b>b V</b>
<p>
    Matrices  
      <b>e(b)</b>           coefficient vector
      <b>e(V)</b>           variance-covariance matrix of the estimators
<p>
    Functions 
      <b>e(sample)</b>      marks estimation sample
<p>
<p>
<b><u>Examples</u></b>
<p>
    Setup
<p>
<b>. webuse auto</b>{p_end}
<p>
    Robust regression with default efficiency
    <b>.  xi: mmregress price mpg headroom trunk weight length turn displacement</b>
    <b>gear_ratio i.rep78 foreign</b>
<p>
<i>(</i><i>click to run</i><i>)</i>
<p>
    <b>.  xi: mmregress price mpg headroom trunk weight length turn displacement</b>
    <b>gear_ratio i.rep78 foreign, initial</b> <i>(</i><i>click to run</i><i>)</i> {pstd} {pstd}Same as
    above, but calling the initial S-estimator{p_end} {pstd} {pstd}Same as
    above, but fixing the Gaussian efficiency to 95%{p_end} {phang2}<b>. xi:</b>
    <b>mmregress price mpg headroom trunk weight length turn displacement</b>
    <b>gear_ratio i.rep78 foreign, eff(0.95)</b> {pstd}Same as above, but starting
    the algorithm with an MS-estimator instead of an S-estimator{p_end}
    {phang2}<b>. xi: mmregress price mpg headroom trunk weight length turn</b>
    <b>displacement gear_ratio, dummies(i.rep78 foreign)</b> {pstd}Same as above,
    but calling the initial MS-estimator rather than the more efficient
    MM-estimator{p_end} {phang2}<b>. xi: mmregress price mpg headroom trunk</b>
    <b>weight length turn displacement gear_ratio, dummies(i.rep78 foreign)</b>
    <b>initial</b> {pstd}Robust fixed effects regression{p_end} {phang2}<b>. use</b>
    <b>http://fmwww.bc.edu/ec-p/data/wooldridge2k/CORNWELL, clear</b> {phang2}<b>. gen</b>
    <b>lncrmrte=ln(crmrte)</b> {phang2}<b>. xi: mmregress lncrmrte prbarr prbconv</b>
    <b>prbpris avgsen, dummies(i.county i.year)</b> {pstd} References {pstd}Dehon,
    C., Gassner, M. and Verardi, V. (2008), "Beware of "Good" Outliers and
    Overoptimistic Conclusions", forthcoming in the Oxford Bulletin of
    Economics and Statistics {pstd}Rousseeuw, P. J. and Yohai, V. (1987),
    "Robust Regression by Means of S-estimators", in Robust and Nonlinear
    Time Series Analysis, edited by J. Franke, W. Härdle and D. Martin,
    Lecture Notes in Statistics No. 26, Springer Verlag, Berlin, pp. 256-272.
    {pstd}Rousseeuw, P. J. and van Zomeren, B. (1990), "Unmasking
    Multivariate Outliers and Leverage Points", Journal of the American
    Statistical Association, 85, pp. 633-639.  {pstd}Salibian-Barrera, M. and
    Yohai, V. (2006). "A fast algorithm for S-regression estimates". Journal
    of Computational and Graphical Statistics, 15, 414-427.  Also see {psee}
    Online:  <b>[R] qreg</b>, <b>[R] regress</b>;{break} <b>[R] rreg</b>, mregress, sregress, 
</pre>