<pre>
-------------------------------------------------------------------------------
<b>help: </b><b>ridgereg</b>                                                   <b>dialog:</b> <b>ridger</b>
<b>&gt; eg</b>
-------------------------------------------------------------------------------
<p>
<b>        </b>+-------+
<b>    </b>----+<b> Title </b>+------------------------------------------------------------
<p>
<b>ridgereg: OLS-Ridge Regression Models and Diagnostic Tests</b>
<p>
<a name="00"></a><b>        </b>+-------------------+
<b>    </b>----+<b> Table of Contents </b>+------------------------------------------------
<p>
     <b>Syntax</b>
     <b>Description</b>
     <b>Ridge Model Options</b>
     <b>Weight Options</b>
     <b>Weighted Variable Type Options</b>
     <b>Options</b>
     <b>Model Selection Diagnostic Criteria</b>
     <b>Multicollinearity Diagnostic Tests</b>
     <b>Saved Results</b>
     <b>References</b>
<p>
 *** <b>Examples</b>
<p>
     <b>Author</b>
<p>
<a name="01"></a><b>        </b>+--------+
<b>    </b>----+<b> Syntax </b>+-----------------------------------------------------------
<p>
     <b>ridgereg</b> <i>depvar</i> <i>indepvars</i> [<i>if</i>] [<i>in</i>] , <b>model(</b><i>orr</i>|<i>grr1</i>|<i>grr2</i>|<i>grr3</i><b>)</b>
 
   [ <b>weights(</b><i>yh</i>|<i>yh2</i>|<i>abse</i>|<i>e2</i>|<i>le2</i>|<i>x</i>|<i>xi</i>|<i>x2</i>|<i>xi2</i><b>)</b> <b><u>wv</u></b><b>ar(</b><i>varname</i><b>)</b>
 
     <b>kr(</b><i>#</i><b>)</b> <b>lmcol</b> <b>diag dn tolog</b> <b>mfx(</b><i>lin</i><b>,</b><i> log</i><b>)</b> <b><u>nocons</u></b><b>tant</b>
 
     <b><u>pred</u></b><b>ict(</b><i>new_var</i><b>)</b> <b><u>res</u></b><b>id(</b><i>new_var</i><b>)</b> <b>coll</b> <b><u>l</u></b><b>evel(</b><i>#</i><b>)</b> ]
<p>
<a name="02"></a><b>        </b>+-------------+
<b>    </b>----+<b> Description </b>+------------------------------------------------------
<p>
    <b>ridgereg</b> estimates (OLS-Ridge Regression models, and computes many tests,
    i.e., Mmulticollinearity Tests, and Model Selection Diagnostic Criteria,
    and Marginal Effects and Elasticities.
<p>
   R2, R2 Adjusted, and F-Test, are obtained from 4 ways:
 
     1- (Buse 1973) R2.
     2- Raw Moments R2.
     3- squared correlation between predicted (Yh) and observed dependent
    variable (Y).
     4- Ratio of variance between predicted (Yh) and observed dependent
    variable (Y).
<p>
     - Adjusted R2: R2_a=1-(1-R2)*(N-1)/(N-K-1).
     - F-Test=R2/(1-R2)*(N-K-1)/(K).
<p>
<a name="03"></a><b>        </b>+---------------------+
<b>    </b>----+<b> Ridge Model Options </b>+----------------------------------------------
<p>
   <b>kr(</b><i>#</i><b>)</b> Ridge k value, must be in the range (0 &lt; k &lt; 1).
<p>
   IF <b>kr(0)</b> in <b>ridge(</b><i>orr</i><b>,</b><i> grr1</i><b>,</b><i> grr2</i><b>,</b><i> grr3</i><b>)</b>, the model will be an OLS
      regression.
<p>
  <b>model(</b><i>orr</i><b>)</b> : Ordinary Ridge Regression    [Judge,et al(1988,p.878) eq.21.4.2]
&gt; .
  <b>model(</b><i>grr1</i><b>)</b>: Generalized Ridge Regression [Judge,et al(1988,p.881) eq.21.4.12
&gt; ].
  <b>model(</b><i>grr2</i><b>)</b>: Iterative Generalized Ridge  [Judge,et al(1988,p.881) eq.21.4.12
&gt; ].
  <b>model(</b><i>grr3</i><b>)</b>: Adaptive Generalized Ridge   [Strawderman(1978)].
<p>
  <b>ridgereg</b> estimates Ordinary Ridge regression as a multicollinearity
    remediation method.
  General form of Ridge Coefficients and Covariance Matrix are:
<p>
  <b>Br = inv[X'X + kI] X'Y</b>
<p>
  <b>Cov=Sig^2 * inv[X'X + kI] (X'X) inv[X'X + kI]</b>
<p>
where:
    Br = Ridge Coefficients Vector (k x 1).
   Cov = Ridge Covariance Matrix (k x k).
     Y = Dependent Variable Vector (N x 1).
     X = Independent Variables Matrix (N x k).
     k = Ridge Value (0 &lt; k &lt; 1).
     I = Diagonal Matrix of Cross Product Matrix (Xs'Xs).
    Xs = Standardized Variables Matrix in Deviation from Mean. 
  Sig2 = (Y-X*Br)'(Y-X*Br)/DF
<p>
<a name="04"></a><b>        </b>+----------------+
<b>    </b>----+<b> Weight Options </b>+---------------------------------------------------
<p>
    <b>wvar(</b><i>varname</i><b>)</b>     Weighted Variable Name
<p>
<a name="05"></a><b>        </b>+--------------------------------+
<b>    </b>----+<b> Weighted Variable Type Options </b>+-----------------------------------
<p>
    <i>weights Options</i>   Description
<p>
    <b>weights(</b><i>yh</i><b>)</b>       Yh - Predicted Value
    <b>weights(</b><i>yh2</i><b>)</b>      Yh^2 - Predicted Value Squared
    <b>weights(</b><i>abse</i><b>)</b>     abs(E) - Absolute Value of Residual
    <b>weights(</b><i>e2</i><b>)</b>       E^2 - Residual Squared
    <b>weights(</b><i>le2</i><b>)</b>      log(E^2) - Log Residual Squared
    <b>weights(</b><i>x</i><b>)</b>        (x) Variable
    <b>weights(</b><i>xi</i><b>)</b>       (1/x) Inverse Variable
    <b>weights(</b><i>x2</i><b>)</b>       (x^2) Squared Variable
    <b>weights(</b><i>xi2</i><b>)</b>      (1/x^2) Inverse Squared Variable
<p>
<a name="06"></a><b>        </b>+---------+
<b>    </b>----+<b> Options </b>+----------------------------------------------------------
<p>
  <b>dn</b>               Use (N) divisor instead of (N-K) for Degrees of Freedom (DF)
<p>
  <b><u>nocons</u></b><b>tant</b>       Exclude Constant Term from Equation
<p>
  <b><u>pred</u></b><b>ict(</b><i>new_var</i><b>)</b>}Predicted values variable
<p>
  <b><u>res</u></b><b>id(</b><i>new_var</i><b>)</b>}Residuals values variable
<p>
  <b>mfx(</b><i>lin</i><b>,</b><i> log</i><b>)</b>    functional form: Linear model <b>(lin)</b>, or Log-Log model <b>(log)</b>,
                   to compute Marginal Effects and Elasticities
   - In Linear model: marginal effects are the coefficients (Bm),
        and elasticities are (Es = Bm X/Y).
   - In Log-Log model: elasticities are the coefficients (Es),
        and the marginal effects are (Bm = Es Y/X).
     - <b>mfx(</b><i>log</i><b>)</b> and <b>tolog</b> options must be combined, to transform variables to l
&gt; og form.
<p>
  <b>tolog</b>            Convert dependent and independent variables
                   to LOG Form in the memory for Log-Log regression.
                   <b>tolog</b> Transforms <i>depvar</i> and <i>indepvars</i>
                   to Log Form without lost the original data variables
<p>
  <b>coll</b>             keep collinear variables; default is removing collinear vari
&gt; ables.
<p>
<a name="07"></a><b>        </b>+-------------------------------------+
<b>    </b>----+<b> Model Selection Diagnostic Criteria </b>+------------------------------
<p>
<b>diag</b>                          Model Selection Diagnostic Criteria
<p>
        - Log Likelihood Function                   LLF
        - Akaike Information Criterion              (1974) AIC
        - Akaike Information Criterion              (1973) Log AIC
        - Schwarz Criterion                         (1978) SC
        - Schwarz Criterion                         (1978) Log SC
        - Amemiya Prediction Criterion              (1969) FPE
        - Hannan-Quinn Criterion                    (1979) HQ
        - Rice Criterion                            (1984) Rice
        - Shibata Criterion                         (1981) Shibata
        - Craven-Wahba Generalized Cross Validation (1979) GCV
<p>
<a name="08"></a><b>        </b>+------------------------------------+
<b>    </b>----+<b> Multicollinearity Diagnostic Tests </b>+-------------------------------
<p>
<b>lmcol</b>                         Multicollinearity Diagnostic Tests
        * Correlation Matrix
        * Multicollinearity Diagnostic Criteria
        * Farrar-Glauber Multicollinearity Tests
        Ho: No Multicollinearity - Ha: Multicollinearity
        * (1) Farrar-Glauber Multicollinearity Chi2-Test
        * (2) Farrar-Glauber Multicollinearity F-Test
        * (3) Farrar-Glauber Multicollinearity t-Test
        * Multicollinearity Ranges
        * Determinant of |X'X|
        * Theil R2 Multicollinearity Effect:
        - Gleason-Staelin Q0
        - Heo Range  Q1
<p>
<b>- Multicollinearity Detection:</b>
  1. A high F statistic or R2 leads to reject the joint hypothesis that all
    of the coefficients are zero, but individual t-statistics are low.
 
  2. High simple correlation coefficients are sufficient but not necessary
    for multicollinearity.
 
  3. One can compute condition number. That is, the ratio of largest to
    smallest root of the matrix x'x. This may not always be useful as the
    standard errors of the estimates depend on the ratios of elements of
    characteristic vectors to the roots.
 
<p>
<b>- Multicollinearity Remediation:</b>
  1. Use prior information or restrictions on the coefficients. One clever
    way to do this was developed by Theil and Goldberger. See <b>tgmixed</b>, and
    Theil(1971, pp 347-352).
 
  2. Use additional data sources. This does not mean more of the same. It
    means pooling cross section and time series.
 
  3. Transform the data. For example, inversion or differencing.
 
  4. Use a principal components estimator. This involves using a weighted
    average of the regressors, rather than all of the regressors.
 
  5. Another alternative regression technique is ridge regression. This
    involves putting extra weight on the main diagonal of X'X.
 
  6. Dropping troublesome RHS variables. This begs the question of
    specification error.
<p>
<a name="09"></a><b>        </b>+---------------+
<b>    </b>----+<b> Saved Results </b>+----------------------------------------------------
<p>
    <b>ridgereg</b> saves the following in <b>e()</b>:
<p>
*** Model Selection Diagnostic Criteria:
   <b>e(N)</b>            number of observations
   <b>e(r2bu)</b>         R-squared (Buse 1973)
   <b>e(r2bu_a)</b>       R-squared Adj (Buse 1973)
   <b>e(r2raw)</b>        Raw Moments R2
   <b>e(r2raw_a)</b>      Raw Moments R2 Adj
   <b>e(f)</b>            F-test
   <b>e(fp)</b>           F-test P-Value
   <b>e(wald)</b>         Wald-test
   <b>e(waldp)</b>        Wald-test P-Value
   <b>e(r2h)</b>          R2 Between Predicted (Yh) and Observed DepVar (Y)
   <b>e(r2h_a)</b>        Adjusted r2h
   <b>e(fh)</b>           F-test due to r2h
   <b>e(fhp)</b>          F-test due to r2h P-Value
<p>
   <b>e(llf)</b>          Log Likelihood Function                   LLF
   <b>e(aic)</b>          Akaike Information Criterion              (1974) AIC
   <b>e(laic)</b>         Akaike Information Criterion              (1973) Log AIC
   <b>e(sc)</b>           Schwarz Criterion                         (1978) SC
   <b>e(lsc)</b>          Schwarz Criterion                         (1978) Log SC
   <b>e(fpe)</b>          Amemiya Prediction Criterion              (1969) FPE
   <b>e(hq)</b>           Hannan-Quinn Criterion                    (1979) HQ
   <b>e(rice)</b>         Rice Criterion                            (1984) Rice
   <b>e(shibata)</b>      Shibata Criterion                         (1981) Shibata
   <b>e(gcv)</b>          Craven-Wahba Generalized Cross Validation (1979) GCV
<p>
Matrixes
   <b>e(b)</b>            coefficient vector
   <b>e(V)</b>            variance-covariance matrix of the estimators
   <b>e(mfxlin)</b>       Marginal Effect and Elasticity in Lin Form
   <b>e(mfxlog)</b>       Marginal Effect and Elasticity in Log Form
<p>
<a name="10"></a><b>        </b>+------------+
<b>    </b>----+<b> References </b>+-------------------------------------------------------
<p>
    D. Belsley (1991)<b> "Conditioning Diagnostics, Collinearity and Weak Data</b>
        <b>in Regression",</b> <i>John Wiley &amp; Sons, Inc., New York, USA</i>.
<p>
    D. Belsley, E. Kuh, and R. Welsch (1980)<b> "Regression Diagnostics:</b>
        <b>Identifying Influential Data and Sources of Collinearity",</b> <i>John Wiley</i>
        <i>&amp; Sons, Inc., New York, USA</i>.
<p>
    Damodar Gujarati (1995)<b> "Basic Econometrics"</b> <i>3rd Edition, McGraw Hill,</i>
        <i>New York, USA</i>.
<p>
    Evagelia, Mitsaki (2011)<b> "Ridge Regression Analysis of Collinear Data",</b> 
        http://www.stat-athens.aueb.gr/~jpan/diatrives/Mitsaki/chapter2.pdf
<p>
    Farrar, D. and Glauber, R. (1976)<b> "Multicollinearity in Regression</b>
        <b>Analysis: the Problem Revisited",</b> <i>Review of Economics and Statistics,</i>
        <i>49</i>; 92-107.
<p>
    Greene, William (1993)<b> "Econometric Analysis",</b> <i>2nd ed., Macmillan</i>
        <i>Publishing Company Inc., New York, USA</i>; 616-618.
<p>
    Greene, William (2007)<b> "Econometric Analysis",</b> <i>6th ed., Upper Saddle</i>
        <i>River, NJ: Prentice-Hall</i>; 387-388.
<p>
    Hoerl A. E. (1962)<b> "Application of Ridge Analysis to Regression</b>
        <b>Problems",</b> <i>Chemical Engineering Progress, 58</i>; 54-59.
<p>
    Hoerl, A. E. and R. W. Kennard (1970a)<b> "Ridge Regression: Biased</b>
        <b>Estimation for Non-Orthogonal Problems",</b> <i>Technometrics, 12</i>; 55-67.
<p>
    Hoerl, A. E. and R. W. Kennard (1970b)<b> "Ridge Regression: Applications to</b>
        <b>Non-Orthogonal Problems",</b> <i>Technometrics, 12</i>; 69-82.
<p>
    Hoerl, A. E. ,R. W. Kennard, &amp; K. Baldwin (1975)<b> "Ridge Regression: Some</b>
        <b>Simulations",</b> <i>Communications in Statistics, A, 4</i>; 105-123.
<p>
    Hoerl, A. E. and R. W. Kennard (1976)<b> "Ridge Regression: Iterative</b>
        <b>Estimation of the Biasing Parameter",</b> <i>Communications in Statistics,</i>
        <i>A, 5</i>; 77-88.
<p>
    Marquardt D.W. (1970)<b> "Generalized Inverses, Ridge Regression, Biased</b>
        <b>Linear Estimation, and Nonlinear Estimation",</b> <i>Technometrics, 12</i>;
        591-612.
<p>
    Marquardt D.W. &amp; R. Snee (1975)<b> "Ridge Regression in Practice",</b> <i>The</i>
        <i>American Statistician, 29</i>; 3-19.
<p>
    Pidot, George (1969)<b> "A Principal Components Analysis of the Determinants</b>
        <b>of Local Government Fiscal Patterns",</b> <i>Review of Economics and</i>
        <i>Statistics, Vol. 51</i>; 176-188.
<p>
    Rencher, Alvin C. (1998)<b> "Multivariate Statistical Inference and</b>
        <b>Applications",</b> <i>John Wiley &amp; Sons, Inc., New York, USA</i>; 21-22.
<p>
    Strawderman, W. E. (1978)<b> "Minimax Adaptive Generalized Ridge Regression</b>
        <b>Estimators",</b> <i>Journal American Statistical Association, 73</i>; 623-627.
<p>
    Theil, Henri (1971)<b> "Principles of Econometrics",</b> <i>John Wiley &amp; Sons,</i>
        <i>Inc., New York, USA</i>.
<p>
<a name="11"></a><b>        </b>+----------+
<b>    </b>----+<b> Examples </b>+---------------------------------------------------------
<p>
  (1) Example of Ridge regression models,
      is decribed in: [Judge, et al(1988, p.882)], and also Theil R2
      Multicollinearity Effect in: [Judge, et al(1988, p.872)], for
      Klein-Goldberger data.
<p>
        clear all
<p>
        sysuse ridgereg1.dta, clear
<p>
        ridgereg y x1 x2 x3 , model(orr) kr(0.5) mfx(lin) lmcol diag
<p>
        ridgereg y x1 x2 x3 , model(orr) kr(0.5) mfx(lin) weights(x) wvar(x1)
<p>
        ridgereg y x1 x2 x3 , model(grr1) mfx(lin)
<p>
        ridgereg y x1 x2 x3 , model(grr2) mfx(lin)
<p>
        ridgereg y x1 x2 x3 , model(grr3) mfx(lin)
<p>
  (2) Example of Gleason-Staelin, and Heo Multicollinearity Ranges,
      is decribed in: [Rencher(1998, pp. 20-22)].
<p>
        clear all
<p>
        sysuse ridgereg2.dta, clear
<p>
        ridgereg y x1 x2 x3 x4 x5 , model(orr) lmcol
<p>
  (3) Example of Farrar-Glauber Multicollinearity Chi2, F, t Tests
      is decribed in:[Evagelia(2011, chap.2, p.23)].
<p>
        clear all
<p>
        sysuse ridgereg3.dta, clear
<p>
        ridgereg y x1 x2 x3 x4 x5 x6 , model(orr) lmcol
-------------------------------------------------------------------------------
<p>
. clear all
. sysuse ridgereg1.dta , clear
. ridgereg y x1 x2 x3 , model(orr) kr(0) diag lmcol mfx(lin)
<p>
==============================================================================
* (OLS) Ridge Regression - Ordinary Ridge Regression
==============================================================================
  y = x1 + x2 + x3
------------------------------------------------------------------------------
  Ridge k Value     =   0.00000     |   Ordinary Ridge Regression
------------------------------------------------------------------------------
  Sample Size       =          20
  Wald Test         =    322.1130   |   P-Value &gt; Chi2(3)       =      0.0000
  F-Test            =    107.3710   |   P-Value &gt; F(3 , 16)     =      0.0000
 (Buse 1973) R2     =      0.9527   |   Raw Moments R2          =      0.9971
 (Buse 1973) R2 Adj =      0.9438   |   Raw Moments R2 Adj      =      0.9965
  Root MSE (Sigma)  =      4.5272   |   Log Likelihood Function =    -56.3495
------------------------------------------------------------------------------
- R2h= 0.9527   R2h Adj= 0.9438  F-Test =  107.37 P-Value &gt; F(3 , 16)  0.0000
- R2v= 0.9527   R2v Adj= 0.9438  F-Test =  107.37 P-Value &gt; F(3 , 16)  0.0000
------------------------------------------------------------------------------
           y |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
          x1 |   1.058783    .173579     6.10   0.000     .6908121    1.426754
          x2 |   .4522435   .6557569     0.69   0.500    -.9378991    1.842386
          x3 |   .1211505   1.087042     0.11   0.913    -2.183275    2.425576
       _cons |   8.132845   8.921103     0.91   0.375    -10.77905    27.04474
------------------------------------------------------------------------------
<p>
==============================================================================
* OLS Model Selection Diagnostic Criteria - Model= (orr)
==============================================================================
- Log Likelihood Function       LLF             =    -56.3495
- Akaike Final Prediction Error AIC             =     22.1330
- Schwarz Criterion             SC              =     25.6984
- Akaike Information Criterion  ln AIC          =      3.0971
- Schwarz Criterion             ln SC           =      3.2464
- Amemiya Prediction Criterion  FPE             =     23.5700
- Hannan-Quinn Criterion        HQ              =     22.7878
- Rice Criterion                Rice            =     23.4236
- Shibata Criterion             Shibata         =     21.3155
- Craven-Wahba Generalized Cross Validation GCV =     22.6941
------------------------------------------------------------------------------
<p>
==============================================================================
*** Multicollinearity Diagnostic Tests - Model= (orr)
==============================================================================
<p>
* Correlation Matrix
(obs=20)
<p>
             |       x1       x2       x3
-------------+---------------------------
          x1 |   1.0000
          x2 |   0.7185   1.0000
          x3 |   0.9152   0.6306   1.0000
<p>
* Multicollinearity Diagnostic Criteria
+------------------------------------------------------------------------------
&gt; -+
|   Var |  Eigenval |  C_Number |   C_Index |       VIF |     1/VIF |   R2_xi,X
&gt;  |
|-------+-----------+-----------+-----------+-----------+-----------+----------
&gt; -|
|    x1 |    2.5160 |    1.0000 |    1.0000 |    7.7349 |    0.1293 |    0.8707
&gt;  |
|    x2 |    0.4081 |    6.1651 |    2.4830 |    2.0862 |    0.4793 |    0.5207
&gt;  |
|    x3 |    0.0758 |   33.1767 |    5.7599 |    6.2127 |    0.1610 |    0.8390
&gt;  |
+------------------------------------------------------------------------------
&gt; -+
<p>
* Farrar-Glauber Multicollinearity Tests
  Ho: No Multicollinearity - Ha: Multicollinearity
--------------------------------------------------
<p>
* (1) Farrar-Glauber Multicollinearity Chi2-Test:
    Chi2 Test =   43.8210    P-Value &gt; Chi2(3) 0.0000
<p>
* (2) Farrar-Glauber Multicollinearity F-Test:
+--------------------------------------------------------+
|   Variable |   F_Test |      DF1 |      DF2 |  P_Value |
|------------+----------+----------+----------+----------|
|         x1 |   57.246 |   17.000 |    3.000 |    0.003 |
|         x2 |    9.233 |   17.000 |    3.000 |    0.046 |
|         x3 |   44.308 |   17.000 |    3.000 |    0.005 |
+--------------------------------------------------------+
<p>
* (3) Farrar-Glauber Multicollinearity t-Test:
+-------------------------------------+
| Variable |     x1 |     x2 |     x3 |
|----------+--------+--------+--------|
|       x1 |      . |        |        |
|       x2 |  4.259 |      . |        |
|       x3 |  9.362 |  3.350 |      . |
+-------------------------------------+
<p>
* |X'X| Determinant:
  |X'X| = 0 Multicollinearity - |X'X| = 1 No Multicollinearity
  |X'X| Determinant:       (0 &lt; 0.0779 &lt; 1)
---------------------------------------------------------------
<p>
* Theil R2 Multicollinearity Effect:
  R2 = 0 No Multicollinearity - R2 = 1 Multicollinearity
     - Theil R2:           (0 &lt; 0.8412 &lt; 1)
---------------------------------------------------------------
<p>
* Multicollinearity Range:
  Q = 0 No Multicollinearity - Q = 1 Multicollinearity
     - Gleason-Staelin Q0: (0 &lt; 0.7641 &lt; 1)
    1- Heo Range Q1:       (0 &lt; 0.8581 &lt; 1)
    2- Heo Range Q2:       (0 &lt; 0.8129 &lt; 1)
    3- Heo Range Q3:       (0 &lt; 0.7209 &lt; 1)
    4- Heo Range Q4:       (0 &lt; 0.7681 &lt; 1)
    5- Heo Range Q5:       (0 &lt; 0.8798 &lt; 1)
    6- Heo Range Q6:       (0 &lt; 0.7435 &lt; 1)
------------------------------------------------------------------------------
<p>
* Marginal Effect - Elasticity (Model= orr): Linear *
<p>
+---------------------------------------------------------------------------+
|   Variable | Marginal_Effect(B) |     Elasticity(Es) |               Mean |
|------------+--------------------+--------------------+--------------------|
|         x1 |             1.0588 |             0.7683 |            52.5840 |
|         x2 |             0.4522 |             0.1106 |            17.7245 |
|         x3 |             0.1212 |             0.0088 |             5.2935 |
+---------------------------------------------------------------------------+
 Mean of Dependent Variable =     72.4650
<p>
<a name="12"></a><b>        </b>+--------+
<b>    </b>----+<b> Author </b>+-----------------------------------------------------------
<p>
  <b>Emad Abd Elmessih Shehata</b>
  <b>Professor (PhD Economics)</b>
  <b>Agricultural Research Center - Agricultural Economics Research Institute - Eg</b>
<b>&gt; ypt</b>
  <b>Email: </b>emadstat@hotmail.com
  <b>WebPage:                </b>http://emadstat.110mb.com/stata.htm
  <b>WebPage at IDEAS:       </b>http://ideas.repec.org/f/psh494.html
  <b>WebPage at EconPapers:  </b>http://econpapers.repec.org/RAS/psh494.htm
<p>
<b>        </b>+-------------------+
<b>    </b>----+<b> RIDGEREG Citation </b>+------------------------------------------------
<p>
 <b>Shehata, Emad Abd Elmessih (2012)</b>
 <b>RIDGEREG: "OLS-Ridge Regression Models and Diagnostic Tests"</b>
<p>
        http://ideas.repec.org/c/boc/bocode/s457347.html
<p>
        http://econpapers.repec.org/software/bocbocode/s457347.htm
<p>
<p>
<b><u>Online Help:</u></b>
<p>
<b>* Econometric Regression Models:</b>
<p>
<b>* (1) (OLS) * Ordinary Least Squares Regression Models:</b>
<b>olsreg</b>     OLS Econometric Ridge &amp; Weighted Regression Models: Stata Module Too
&gt; lkit
<b>ridgereg</b>   OLS Ridge Regression Models
<b>gmmreg</b>     OLS Generalized Method of Moments (GMM): Ridge &amp; Weighted Regression
<b>chowreg</b>    OLS Structural Change Regressions and Chow Test
---------------------------------------------------------------------------
<b>* (2) (2SLS-IV) * Two-Stage Least Squares &amp; Instrumental Variables Regression M</b>
<b>&gt; odels:</b>
<b>reg2</b>       2SLS-IV Econometric Ridge &amp; Weighted Regression Models: Stata Module
&gt;  Toolkit
<b>gmmreg2</b>    2SLS-IV Generalized Method of Moments (GMM): Ridge &amp; Weighted Regres
&gt; sion
<b>limlreg2</b>   Limited-Information Maximum Likelihood (LIML) IV Regression
<b>meloreg2</b>   Minimum Expected Loss (MELO) IV Regression
<b>ridgereg2</b>  Ridge 2SLS-LIML-GMM-MELO-Fuller-kClass IV Regression
<b>ridge2sls</b>  Two-Stage Least Squares Ridge Regression
<b>ridgegmm</b>   Generalized Method of Moments (GMM) IV Ridge Regression
<b>ridgeliml</b>  Limited-Information Maximum Likelihood (LIML) IV Ridge Regression
<b>ridgemelo</b>  Minimum Expected Loss (MELO) IV Ridge Regression
---------------------------------------------------------------------------
<b>* (3) * Panel Data Regression Models:</b>
<b>regxt</b>      Panel Data Econometric Ridge &amp; Weighted Regression Models: Stata Mod
&gt; ule Toolkit
<b>xtregdhp</b>   Han-Philips (2010) Linear Dynamic Panel Data Regression
<b>xtregam</b>    Amemiya Random-Effects Panel Data: Ridge &amp; Weighted Regression
<b>xtregbem</b>   Between-Effects Panel Data: Ridge &amp; Weighted Regression
<b>xtregbn</b>    Balestra-Nerlove Random-Effects Panel Data: Ridge &amp; Weighted Regress
&gt; ion
<b>xtregfem</b>   Fixed-Effects Panel Data: Ridge &amp; Weighted Regression
<b>xtregmle</b>   Trevor Breusch MLE Random-Effects Panel Data: Ridge &amp; Weighted Regre
&gt; ssion
<b>xtregrem</b>   Fuller-Battese GLS Random-Effects Panel Data: Ridge &amp; Weighted Regre
&gt; ssion
<b>xtregsam</b>   Swamy-Arora Random-Effects Panel Data: Ridge &amp; Weighted Regression
<b>xtregwem</b>   Within-Effects Panel Data: Ridge &amp; Weighted Regression
<b>xtregwhm</b>   Wallace-Hussain Random-Effects Panel Data: Ridge &amp; Weighted Regressi
&gt; on
<b>xtreghet</b>   MLE Random-Effects Multiplicative Heteroscedasticity Panel Data Regr
&gt; ession
---------------------------------------------------------------------------
<b>* (4) (MLE) * Maximum Likelihood Estimation Regression Models:</b>
<b>mlereg</b>     MLE Econometric Regression Models: Stata Module Toolkit
<b>mleregn</b>    MLE Normal Regression
<b>mleregln</b>   MLE Log Normal Regression
<b>mlereghn</b>   MLE Half Normal Regression
<b>mlerege</b>    MLE Exponential Regression
<b>mleregle</b>   MLE Log Exponential Regression
<b>mleregg</b>    MLE Gamma Regression
<b>mlereglg</b>   MLE Log Gamma Regression
<b>mlereggg</b>   MLE Generalized Gamma Regression
<b>mlereglgg</b>  MLE Log Generalized Gamma Regression
<b>mleregb</b>    MLE Beta Regression
<b>mleregev</b>   MLE Extreme Value Regression
<b>mleregw</b>    MLE Weibull Regression
<b>mlereglw</b>   MLE Log Weibull Regression
<b>mleregilg</b>  MLE Inverse Log Gauss Regression
---------------------------------------------------------------------------
<b>* (5) * Autocorrelation Regression Models:</b>
<b>autoreg</b>    Autoregressive Least Squares Regression Models: Stata Module Toolkit
<b>alsmle</b>     Beach-Mackinnon AR(1) Autoregressive Maximum Likelihood Estimation R
&gt; egression
<b>automle</b>    Beach-Mackinnon AR(1) Autoregressive Maximum Likelihood Estimation R
&gt; egression
<b>autopagan</b>  Pagan AR(p) Conditional Autoregressive Least Squares Regression
<b>autoyw</b>     Yule-Walker AR(p) Unconditional Autoregressive Least Squares Regress
&gt; ion
<b>autopw</b>     Prais-Winsten AR(p) Autoregressive Least Squares Regression
<b>autoco</b>     Cochrane-Orcutt AR(p) Autoregressive Least Squares Regression
<b>autofair</b>   Fair AR(1) Autoregressive Least Squares Regression
---------------------------------------------------------------------------
<b>* (6) * Heteroscedasticity Regression Models:</b>
<b>hetdep</b>     MLE Dependent Variable Heteroscedasticity
<b>hetmult</b>    MLE Multiplicative Heteroscedasticity Regression
<b>hetstd</b>     MLE Standard Deviation Heteroscedasticity Regression
<b>hetvar</b>     MLE Variance Deviation Heteroscedasticity Regression
<b>glsreg</b>     Generalized Least Squares Regression
---------------------------------------------------------------------------
<b>* (7) * Non Normality Regression Models:</b>
<b>robgme</b>     MLE Robust Generalized Multivariate Error t Distribution
<b>bcchreg</b>    Classical Box-Cox Multiplicative Heteroscedasticity Regression
<b>bccreg</b>     Classical Box-Cox Regression
<b>bcereg</b>     Extended Box-Cox Regression
---------------------------------------------------------------------------
<b>* (8) (NLS) * Nonlinear Least Squares Regression Regression Models:</b>
<b>autonls</b>    Non Linear Autoregressive Least Squares Regression
<b>qregnls</b>    Non Linear Quantile Regression
---------------------------------------------------------------------------
<b>* (9) * Logit Regression Models:</b>
<b>logithetm</b>  Logit Multiplicative Heteroscedasticity Regression
<b>mnlogit</b>    Multinomial Logit Regression
---------------------------------------------------------------------------
<b>* (10) * Probit Regression Models:</b>
<b>probithetm</b> Probit Multiplicative Heteroscedasticity Regression
<b>mnprobit</b>   Multinomial Probit Regression
---------------------------------------------------------------------------
<b>* (11) * Tobit Regression Models:</b>
<b>tobithetm</b>  Tobit Multiplicative Heteroscedasticity Regression 
---------------------------------------------------------------------------
<b>* Multicollinearity Tests:</b>
<b>lmcol</b>      OLS Multicollinearity Diagnostic Tests
<b>fgtest</b>     Farrar-Glauber Multicollinearity Chi2, F, t Tests
<b>theilr2</b>    Theil R2 Multicollinearity Effect
---------------------------------------------------------------------------
<p>
</pre>